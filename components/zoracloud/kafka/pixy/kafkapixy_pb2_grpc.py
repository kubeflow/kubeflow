# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!
"""Client and server classes corresponding to protobuf-defined services."""
import grpc

import kafkapixy_pb2 as kafkapixy__pb2


class KafkaPixyStub(object):
    """Missing associated documentation comment in .proto file."""

    def __init__(self, channel):
        """Constructor.

        Args:
            channel: A grpc.Channel.
        """
        self.Produce = channel.unary_unary(
                '/KafkaPixy/Produce',
                request_serializer=kafkapixy__pb2.ProdRq.SerializeToString,
                response_deserializer=kafkapixy__pb2.ProdRs.FromString,
                )
        self.ConsumeNAck = channel.unary_unary(
                '/KafkaPixy/ConsumeNAck',
                request_serializer=kafkapixy__pb2.ConsNAckRq.SerializeToString,
                response_deserializer=kafkapixy__pb2.ConsRs.FromString,
                )
        self.Ack = channel.unary_unary(
                '/KafkaPixy/Ack',
                request_serializer=kafkapixy__pb2.AckRq.SerializeToString,
                response_deserializer=kafkapixy__pb2.AckRs.FromString,
                )
        self.GetOffsets = channel.unary_unary(
                '/KafkaPixy/GetOffsets',
                request_serializer=kafkapixy__pb2.GetOffsetsRq.SerializeToString,
                response_deserializer=kafkapixy__pb2.GetOffsetsRs.FromString,
                )
        self.SetOffsets = channel.unary_unary(
                '/KafkaPixy/SetOffsets',
                request_serializer=kafkapixy__pb2.SetOffsetsRq.SerializeToString,
                response_deserializer=kafkapixy__pb2.SetOffsetsRs.FromString,
                )
        self.ListTopics = channel.unary_unary(
                '/KafkaPixy/ListTopics',
                request_serializer=kafkapixy__pb2.ListTopicRq.SerializeToString,
                response_deserializer=kafkapixy__pb2.ListTopicRs.FromString,
                )
        self.ListConsumers = channel.unary_unary(
                '/KafkaPixy/ListConsumers',
                request_serializer=kafkapixy__pb2.ListConsumersRq.SerializeToString,
                response_deserializer=kafkapixy__pb2.ListConsumersRs.FromString,
                )
        self.GetTopicMetadata = channel.unary_unary(
                '/KafkaPixy/GetTopicMetadata',
                request_serializer=kafkapixy__pb2.GetTopicMetadataRq.SerializeToString,
                response_deserializer=kafkapixy__pb2.GetTopicMetadataRs.FromString,
                )


class KafkaPixyServicer(object):
    """Missing associated documentation comment in .proto file."""

    def Produce(self, request, context):
        """Produce writes a message to a Kafka topic.

        If ProdReq.async_mode is false (default value) then the request will
        block until the message is written to all ISR. In this case the respose
        will contain the partition and offset of the message. This has to be
        used to achive at-least-once deliverability guarantee.
        If ProdReq.async_mode is true, then Kafka-Pixy returns immediately after
        it gets the request and performs write on the backgroud. This mode
        ensures highest throughput but messages can be lost, e.g. if the host
        crashes before Kafka-Pixy has a chance to complete write.

        Hash of ProdReq.key_value is used to determine a partition that the
        message should be written to. If you want a message to go to an random
        partition then set ProdReq.key_undefined to true. Note that if both
        ProdReq.key_undefined and ProdReq.key_value are left default, which is
        empty string and false respectively, then messages will be consitently
        written to a partiticular partition selected by the hash of an empty
        string.

        gRPC error codes:
        * Invalid Argument (3): see the status description for details;
        * Internal (13): see the status description and logs for details;
        * Unavailable (14): the service is shutting down.
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def ConsumeNAck(self, request, context):
        """Consume reads a message from a topic and optionally acknowledges a
        message previously consumed from the same topic.

        Requests are performed in long polling fation, that is if all available
        messages have been consumed then the request will block for
        config.yaml:proxies.<cluster>.consumer.long_polling_timeout waiting for
        new messages. If no new messages is produced while waiting the request
        will return gRPC error with 408 status code.

        To consume the first message set ConsNAckReq.no_ack to true, since there
        is no message to acknowledge at this point. In the second and all
        subsequent calls of the method set ConsNAckReq.ack_partition and
        ConsNAckReq.ack_offset to the respective values of ConsRes returned by
        the previous method call. To acknowledge the last consumed message before
        teminating the application call Ack method.

        If a message is not acknowledged within
        config.yaml:proxies.<cluster>.consumer.ack_timeout the it will be returned
        by Kafka-Pixy in ConsRes again possibly to another application.

        If at-least-once delivery guarantee and retries are not desirable, then
        you can set ConsNAckReq.auto_ack to true and Kafka-Pixy will acknowledge
        messages automatically before returning them in ConsRes.

        gRPC error codes:
        * Not Found (5): It just means that all message has been consumed and
        the long polling timeout has elaspsed. Just keep calling this method
        in a loop;
        * Resource Exhausted (8): too many consume requests. Either reduce the
        number of consuming threads or increase
        config.yaml:proxies.<cluster>.consumer.channel_buffer_size;
        * Invalid Argument (3): see the status description for details;
        * Internal (13): see the status description and logs for details;
        * Unavailable (14): the service is shutting down.
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def Ack(self, request, context):
        """Ack acknowledges a message earlier consumed from a topic.

        This method is provided solely to acknowledge the last consumed message
        before the application terminates. In all other cases ConsumeNAck should
        be used.

        gRPC error codes:
        * Invalid Argument (3): see the status description for details;
        * Internal (13): see the status description and logs for details;
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def GetOffsets(self, request, context):
        """Fetches partition offsets for the specified topic and group

        gRPC error codes:
        * Invalid Argument (3): If unable to find the cluster named in the request
        * Internal (13): If Kafka returns an error on offset request
        * NotFound (5): If the group and or topic does not exist
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def SetOffsets(self, request, context):
        """Sets partition offsets for the specified topic and group.
        NOTE: Although the request accepts the PartitionOffset object i
        only 'Partition', 'Offset' and 'Metadata' are set by this method

        gRPC error codes:
        * Invalid Argument (3): If unable to find the cluster named in the request
        * Internal (13): If Kafka returns an error on offset request
        * NotFound (5): If the group and or topic does not exist
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def ListTopics(self, request, context):
        """Lists all topics and metadata with optional metadata for the partitions of the topic

        gRPC error codes:
        * Invalid Argument (3): If unable to find the cluster named in the request
        * Internal (13): If Kafka returns an error on request
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def ListConsumers(self, request, context):
        """Lists all consumers of a topic

        gRPC error codes:
        * Invalid Argument (3): If unable to find the cluster named in the request
        * Internal (13): If Kafka returns an error on request
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')

    def GetTopicMetadata(self, request, context):
        """Fetches topic metadata and optional metadata for the partitions of the topic

        gRPC error codes:
        * Invalid Argument (3): If unable to find the cluster named in the request
        * Internal (13): If Kafka returns an error on request
        * NotFound (5): If the topic does not exist
        """
        context.set_code(grpc.StatusCode.UNIMPLEMENTED)
        context.set_details('Method not implemented!')
        raise NotImplementedError('Method not implemented!')


def add_KafkaPixyServicer_to_server(servicer, server):
    rpc_method_handlers = {
            'Produce': grpc.unary_unary_rpc_method_handler(
                    servicer.Produce,
                    request_deserializer=kafkapixy__pb2.ProdRq.FromString,
                    response_serializer=kafkapixy__pb2.ProdRs.SerializeToString,
            ),
            'ConsumeNAck': grpc.unary_unary_rpc_method_handler(
                    servicer.ConsumeNAck,
                    request_deserializer=kafkapixy__pb2.ConsNAckRq.FromString,
                    response_serializer=kafkapixy__pb2.ConsRs.SerializeToString,
            ),
            'Ack': grpc.unary_unary_rpc_method_handler(
                    servicer.Ack,
                    request_deserializer=kafkapixy__pb2.AckRq.FromString,
                    response_serializer=kafkapixy__pb2.AckRs.SerializeToString,
            ),
            'GetOffsets': grpc.unary_unary_rpc_method_handler(
                    servicer.GetOffsets,
                    request_deserializer=kafkapixy__pb2.GetOffsetsRq.FromString,
                    response_serializer=kafkapixy__pb2.GetOffsetsRs.SerializeToString,
            ),
            'SetOffsets': grpc.unary_unary_rpc_method_handler(
                    servicer.SetOffsets,
                    request_deserializer=kafkapixy__pb2.SetOffsetsRq.FromString,
                    response_serializer=kafkapixy__pb2.SetOffsetsRs.SerializeToString,
            ),
            'ListTopics': grpc.unary_unary_rpc_method_handler(
                    servicer.ListTopics,
                    request_deserializer=kafkapixy__pb2.ListTopicRq.FromString,
                    response_serializer=kafkapixy__pb2.ListTopicRs.SerializeToString,
            ),
            'ListConsumers': grpc.unary_unary_rpc_method_handler(
                    servicer.ListConsumers,
                    request_deserializer=kafkapixy__pb2.ListConsumersRq.FromString,
                    response_serializer=kafkapixy__pb2.ListConsumersRs.SerializeToString,
            ),
            'GetTopicMetadata': grpc.unary_unary_rpc_method_handler(
                    servicer.GetTopicMetadata,
                    request_deserializer=kafkapixy__pb2.GetTopicMetadataRq.FromString,
                    response_serializer=kafkapixy__pb2.GetTopicMetadataRs.SerializeToString,
            ),
    }
    generic_handler = grpc.method_handlers_generic_handler(
            'KafkaPixy', rpc_method_handlers)
    server.add_generic_rpc_handlers((generic_handler,))


 # This class is part of an EXPERIMENTAL API.
class KafkaPixy(object):
    """Missing associated documentation comment in .proto file."""

    @staticmethod
    def Produce(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/KafkaPixy/Produce',
            kafkapixy__pb2.ProdRq.SerializeToString,
            kafkapixy__pb2.ProdRs.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def ConsumeNAck(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/KafkaPixy/ConsumeNAck',
            kafkapixy__pb2.ConsNAckRq.SerializeToString,
            kafkapixy__pb2.ConsRs.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def Ack(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/KafkaPixy/Ack',
            kafkapixy__pb2.AckRq.SerializeToString,
            kafkapixy__pb2.AckRs.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def GetOffsets(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/KafkaPixy/GetOffsets',
            kafkapixy__pb2.GetOffsetsRq.SerializeToString,
            kafkapixy__pb2.GetOffsetsRs.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def SetOffsets(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/KafkaPixy/SetOffsets',
            kafkapixy__pb2.SetOffsetsRq.SerializeToString,
            kafkapixy__pb2.SetOffsetsRs.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def ListTopics(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/KafkaPixy/ListTopics',
            kafkapixy__pb2.ListTopicRq.SerializeToString,
            kafkapixy__pb2.ListTopicRs.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def ListConsumers(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/KafkaPixy/ListConsumers',
            kafkapixy__pb2.ListConsumersRq.SerializeToString,
            kafkapixy__pb2.ListConsumersRs.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)

    @staticmethod
    def GetTopicMetadata(request,
            target,
            options=(),
            channel_credentials=None,
            call_credentials=None,
            insecure=False,
            compression=None,
            wait_for_ready=None,
            timeout=None,
            metadata=None):
        return grpc.experimental.unary_unary(request, target, '/KafkaPixy/GetTopicMetadata',
            kafkapixy__pb2.GetTopicMetadataRq.SerializeToString,
            kafkapixy__pb2.GetTopicMetadataRs.FromString,
            options, channel_credentials,
            insecure, call_credentials, compression, wait_for_ready, timeout, metadata)